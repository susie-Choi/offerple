"""Collect Exploit Database data."""
from __future__ import annotations

import argparse
import json
import logging
from datetime import datetime
from pathlib import Path

import yaml
from tqdm import tqdm

from zero_day_defense.data_sources.exploit_db import ExploitDBDataSource


logger = logging.getLogger(__name__)


def main():
    parser = argparse.ArgumentParser(description="Collect Exploit-DB data")
    parser.add_argument(
        "config",
        type=Path,
        help="Path to configuration YAML file",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Logging verbosity",
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="Override output file path",
    )

    args = parser.parse_args()
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    # Load configuration
    with args.config.open("r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    cutoff_date = datetime.fromisoformat(config["cutoff_date"])
    output_dir = Path(config.get("output_dir", "data/raw"))
    output_dir.mkdir(parents=True, exist_ok=True)

    # Initialize data source
    exploit_source = ExploitDBDataSource(
        timeout=config.get("request_timeout", 30),
        rate_limit_sleep=config.get("rate_limit_sleep", 1.0),
    )

    # Determine output file
    output_file = args.output or (output_dir / "exploits.jsonl")
    logger.info(f"Output will be written to: {output_file}")

    # Collect mode
    mode = config.get("mode", "all")
    
    if mode == "all":
        logger.info("Collecting all exploits from Exploit-DB...")
        try:
            result = exploit_source.collect("all", cutoff=cutoff_date)
            
            with output_file.open("w", encoding="utf-8") as f:
                record = {
                    "source": result.source,
                    "package": result.package,
                    "collected_at": result.collected_at.isoformat(),
                    "payload": result.payload,
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
            
            exploit_count = result.payload.get("total_results", 0)
            logger.info(f"Collected {exploit_count} exploits")
            
        except Exception as e:
            logger.error(f"Error collecting exploits: {e}")
    
    elif mode == "cve_list":
        cve_list = config.get("cve_list", [])
        logger.info(f"Collecting exploits for {len(cve_list)} CVEs")
        
        collected_count = 0
        error_count = 0
        
        with output_file.open("w", encoding="utf-8") as f:
            for cve_id in tqdm(cve_list, desc="Collecting Exploits"):
                try:
                    result = exploit_source.collect(cve_id, cutoff=cutoff_date)
                    
                    record = {
                        "source": result.source,
                        "package": result.package,
                        "collected_at": result.collected_at.isoformat(),
                        "payload": result.payload,
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
                    f.flush()
                    
                    exploit_count = result.payload.get("total_results", 0)
                    if exploit_count > 0:
                        logger.info(f"{cve_id}: {exploit_count} exploits")
                    collected_count += 1
                    
                except Exception as e:
                    logger.error(f"Error collecting {cve_id}: {e}")
                    error_count += 1
        
        logger.info(f"\n{'='*60}")
        logger.info(f"Collection complete!")
        logger.info(f"  Successfully collected: {collected_count}")
        logger.info(f"  Errors: {error_count}")
    
    logger.info(f"  Output file: {output_file}")


if __name__ == "__main__":
    main()
